{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "class WildlifeTrafficDataPipeline:\n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(os.path.join(data_dir, \"raw\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(data_dir, \"augmented\"), exist_ok=True)\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    # ===================== 1. DATA GENERATION =====================\n",
    "\n",
    "    def _generate_sample_gps_data(self, n_animals=10, days=30, readings_per_day=24):\n",
    "        \"\"\"Generate sample GPS tracking data for wildlife.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        data = []\n",
    "        start_date = datetime.datetime(2023, 1, 1)\n",
    "\n",
    "        # Create base locations for each animal\n",
    "        animal_base_locations = {\n",
    "            f\"A{i}\": {\n",
    "                \"lat\": np.random.uniform(30, 45),\n",
    "                \"lon\": np.random.uniform(-120, -100)\n",
    "            } for i in range(1, n_animals + 1)\n",
    "        }\n",
    "\n",
    "        for animal_id, base_loc in animal_base_locations.items():\n",
    "            direction = 1  # 1 for north, -1 for south\n",
    "\n",
    "            for day in range(days):\n",
    "                if day == days // 2:\n",
    "                    direction = -1\n",
    "\n",
    "                for reading in range(readings_per_day):\n",
    "                    timestamp = start_date + \\\n",
    "                        datetime.timedelta(days=day, hours=reading)\n",
    "\n",
    "                    lat_noise = np.random.normal(0, 0.01)\n",
    "                    lon_noise = np.random.normal(0, 0.01)\n",
    "\n",
    "                    lat = base_loc[\"lat\"] + lat_noise + \\\n",
    "                        (day * 0.01 * direction)\n",
    "                    lon = base_loc[\"lon\"] + lon_noise\n",
    "\n",
    "                    speed = np.random.uniform(0, 5)\n",
    "                    heading = np.random.uniform(0, 360)\n",
    "\n",
    "                    data.append({\n",
    "                        \"animal_id\": animal_id,\n",
    "                        \"species\": np.random.choice([\"deer\", \"elk\", \"bear\", \"wolf\"]),\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"latitude\": lat,\n",
    "                        \"longitude\": lon,\n",
    "                        \"speed\": speed,\n",
    "                        \"heading\": heading,\n",
    "                        \"battery_level\": np.random.uniform(60, 100)\n",
    "                    })\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        self.wildlife_data = df\n",
    "        df.to_csv(os.path.join(self.data_dir, \"raw\",\n",
    "                  \"wildlife_data_raw.csv\"), index=False)\n",
    "        print(f\"Generated wildlife data with shape: {df.shape}\")\n",
    "\n",
    "    def _generate_sample_traffic_data(self, n_segments=20, days=30, readings_per_day=24):\n",
    "        \"\"\"Generate sample traffic density and speed data.\"\"\"\n",
    "        np.random.seed(43)\n",
    "        data = []\n",
    "        start_date = datetime.datetime(2023, 1, 1)\n",
    "\n",
    "        road_segments = {\n",
    "            f\"R{i}\": {\n",
    "                \"start_lat\": np.random.uniform(30, 45),\n",
    "                \"start_lon\": np.random.uniform(-120, -100),\n",
    "                \"end_lat\": np.random.uniform(30, 45),\n",
    "                \"end_lon\": np.random.uniform(-120, -100),\n",
    "                \"road_type\": np.random.choice([\"highway\", \"primary\", \"secondary\", \"local\"]),\n",
    "                \"speed_limit\": np.random.choice([25, 35, 45, 55, 65, 75])\n",
    "            } for i in range(1, n_segments + 1)\n",
    "        }\n",
    "\n",
    "        for segment_id, segment_info in road_segments.items():\n",
    "            for day in range(days):\n",
    "                for reading in range(readings_per_day):\n",
    "                    timestamp = start_date + \\\n",
    "                        datetime.timedelta(days=day, hours=reading)\n",
    "\n",
    "                    if 7 <= reading < 9 or 16 <= reading < 18:\n",
    "                        traffic_factor = np.random.uniform(0.7, 1.0)\n",
    "                    else:\n",
    "                        traffic_factor = np.random.uniform(0.1, 0.6)\n",
    "\n",
    "                    if timestamp.weekday() >= 5:\n",
    "                        traffic_factor *= 0.7\n",
    "\n",
    "                    vehicle_count = int(\n",
    "                        100 * traffic_factor * (1 + np.random.normal(0, 0.1)))\n",
    "                    speed = max(\n",
    "                        segment_info['speed_limit'] * (1 - traffic_factor) * (1 + np.random.normal(0, 0.05)), 0)\n",
    "\n",
    "                    data.append({\n",
    "                        \"road_segment_id\": segment_id,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"start_lat\": segment_info['start_lat'],\n",
    "                        \"start_lon\": segment_info['start_lon'],\n",
    "                        \"end_lat\": segment_info['end_lat'],\n",
    "                        \"end_lon\": segment_info['end_lon'],\n",
    "                        \"road_type\": segment_info['road_type'],\n",
    "                        \"speed_limit\": segment_info['speed_limit'],\n",
    "                        \"vehicle_count\": vehicle_count,\n",
    "                        \"speed\": speed,\n",
    "                        \"weather_condition\": np.random.choice([\"clear\", \"rainy\", \"foggy\", \"snowy\"])\n",
    "                    })\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        self.traffic_data = df\n",
    "        df.to_csv(os.path.join(self.data_dir, \"raw\",\n",
    "                  \"traffic_data_raw.csv\"), index=False)\n",
    "        print(f\"Generated traffic data with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def clean_wildlife_data(self):\n",
    "        if not hasattr(self, 'wildlife_data'):\n",
    "            raise ValueError(\"Wildlife data must be generated first.\")\n",
    "        \n",
    "        df = self.wildlife_data.copy()\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df = df[(df['latitude'] >= -90) & (df['latitude'] <= 90) & \n",
    "                (df['longitude'] >= -180) & (df['longitude'] <= 180)]\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df.sort_values(['animal_id', 'timestamp'], inplace=True)\n",
    "        \n",
    "        self.wildlife_data_cleaned = df\n",
    "        print(f\"Cleaned wildlife data: {df.shape}\")\n",
    "\n",
    "    def clean_traffic_data(self):\n",
    "        if not hasattr(self, 'traffic_data'):\n",
    "            raise ValueError(\"Traffic data must be generated first.\")\n",
    "        \n",
    "        df = self.traffic_data.copy()\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df.sort_values(['road_segment_id', 'timestamp'], inplace=True)\n",
    "        \n",
    "        self.traffic_data_cleaned = df\n",
    "        print(f\"Cleaned traffic data: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_wildlife_data(self, num_synthetic_tracks=100):\n",
    "    if not hasattr(self, 'wildlife_data_cleaned'):\n",
    "        raise ValueError(\"Wildlife data must be cleaned first.\")\n",
    "\n",
    "    synthetic_data = []\n",
    "    animal_ids = self.wildlife_data_cleaned['animal_id'].unique()\n",
    "    for i in range(num_synthetic_tracks):\n",
    "        base_animal_id = np.random.choice(animal_ids)\n",
    "        base_animal_data = self.wildlife_data_cleaned[\n",
    "            self.wildlife_data_cleaned['animal_id'] == base_animal_id\n",
    "        ].copy()\n",
    "\n",
    "        synthetic_track = base_animal_data.copy()\n",
    "        synthetic_track['animal_id'] = f\"synthetic_{base_animal_id}_{i}\"\n",
    "        synthetic_data.append(synthetic_track)\n",
    "\n",
    "    df = pd.concat(synthetic_data, ignore_index=True)\n",
    "    print(f\"Augmented wildlife data: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(self):\n",
    "    if not hasattr(self, 'wildlife_data_cleaned') or not hasattr(self, 'traffic_data_cleaned'):\n",
    "        raise ValueError(\n",
    "                \"Both wildlife and traffic data must be cleaned first.\")\n",
    "\n",
    "    df = pd.merge(\n",
    "        self.wildlife_data_cleaned,\n",
    "        self.traffic_data_cleaned,\n",
    "        on='timestamp',\n",
    "        how='inner'\n",
    "    )\n",
    "    print(f\"Merged dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated wildlife data with shape: (7200, 8)\n",
      "Generated traffic data with shape: (14400, 11)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WildlifeTrafficDataPipeline' object has no attribute 'clean_wildlife_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m pipeline\u001b[38;5;241m.\u001b[39m_generate_sample_gps_data()\n\u001b[0;32m      3\u001b[0m pipeline\u001b[38;5;241m.\u001b[39m_generate_sample_traffic_data()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_wildlife_data\u001b[49m()\n\u001b[0;32m      5\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mclean_traffic_data()\n\u001b[0;32m      6\u001b[0m pipeline\u001b[38;5;241m.\u001b[39maugment_wildlife_data()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WildlifeTrafficDataPipeline' object has no attribute 'clean_wildlife_data'"
     ]
    }
   ],
   "source": [
    "pipeline = WildlifeTrafficDataPipeline()\n",
    "pipeline._generate_sample_gps_data()\n",
    "pipeline._generate_sample_traffic_data()\n",
    "pipeline.clean_wildlife_data()\n",
    "pipeline.clean_traffic_data()\n",
    "pipeline.augment_wildlife_data()\n",
    "pipeline.merge_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
